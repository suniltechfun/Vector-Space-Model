{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Vector-space based IR System\n",
    "## Objectives: \n",
    "### 1. Building Inverted Index\n",
    "### 2. Saving the python dictionaries as pickle files to be used for query optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Its', 'father-in-law', 'I', \"'m\", 'James', 'Leslie', '``', 'Hippo', \"''\", 'Vaughn', '(', 'April', '9', ',', '1888', '–', 'May', '29', ',', '1966', ')', 'Brisbane', \"'s\", 'was', 'an', 'American', 'left-handed']\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = 'Its father-in-law I\\'m James Leslie \"Hippo\" Vaughn (April 9, 1888 – May 29, 1966)  Brisbane\\'s was an American left-handed'\n",
    "tokens = nltk.word_tokenize(cleaned_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from each word\n",
    "def remove_punctuation(words):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    return ''.join(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special not needed characters from each word\n",
    "def remove_special_characters(s):\n",
    "    # removes special characters with ' '    \n",
    "    stripped = re.sub('[^a-zA-z0-9\\s]', '', s)\n",
    "\n",
    "    # Remove underscore\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "    # Change any white space to one space including new line\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "\n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "        return stripped.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text Preprocessing \n",
    "def text_preprocessing(documents_dict):\n",
    "    for key, value in documents_dict.items():\n",
    "        clean_text_1 = remove_punctuation(value)\n",
    "        clean_text_2 = remove_special_characters(clean_text_1)\n",
    "        clean_text_2.replace(\"[(,.]\", \"\")\n",
    "        documents_dict[key] = clean_text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read document file...file used is \"wiki_62\"\n",
    "def read_corpus_file(documents_dict):        \n",
    "        with open('wiki_62', 'r', encoding='utf8') as corpus_file:\n",
    "            corpus_soup = BeautifulSoup(corpus_file.read(), 'html.parser')\n",
    "            for doc in corpus_soup.find_all('doc'):\n",
    "                documents_dict[int(doc['id'])] = doc.get_text()\n",
    "                documents_title_dict[int(doc['id'])] = ''.join(list(doc.get(\"title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make complete bag of words model\n",
    "def make_bag_of_words(bag_of_words, documents_dict):        \n",
    "    for doc_id, doc_text in documents_dict.items():\n",
    "        tokens = nltk.word_tokenize(doc_text)\n",
    "        for token in tokens:\n",
    "            if bag_of_words.get(token):\n",
    "                bag_of_words[token]['doc_ids'].append(doc_id)\n",
    "            else:\n",
    "                bag_of_words[token] = {'doc_ids': [doc_id]}        \n",
    "    for key, value in bag_of_words.items():\n",
    "         bag_of_words[key]['df'] = len(list(set(bag_of_words[key]['doc_ids'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build inverted index using SMART Notation\n",
    "#### SMART Notation used : lnc.ltc(ddd.qqq)\n",
    "#### ltc ==> Logarithmic tf +  NO IDF + Cosine Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building inverted index using dictionary\n",
    "def build_inverted_index(tf_idf_vector, documents_dict, bag_of_words):\n",
    "    documents_count = len(documents_dict.keys())\n",
    "    total_tokens = list(bag_of_words.keys())\n",
    "    \n",
    "    tokens_with_index = {}\n",
    "    for index, token in enumerate(total_tokens):\n",
    "        tokens_with_index[token] = index\n",
    "\n",
    "    for doc_id, doc_text in documents_dict.items():\n",
    "        doc_tokens = nltk.word_tokenize(doc_text)\n",
    "        doc_tokens_counter = Counter(doc_tokens)\n",
    "        doc_tokens_count = len(doc_tokens)\n",
    "        \n",
    "        np_token_array = np.zeros((len(total_tokens)))\n",
    "        tf_idf_vector[doc_id] = {'tf_idf_vector': np_token_array}\n",
    "        cnt = 0\n",
    "        for token, value in bag_of_words.items():\n",
    "            # Adding 1 to avoid divide by Zero warning\n",
    "            #Logarithmic tf \n",
    "            tf = 1 + np.log10(doc_tokens_counter[token] + 1)\n",
    "            cnt += tf * tf\n",
    "        cnt = np.sqrt(cnt)\n",
    "        for token, value in bag_of_words.items():\n",
    "            # Adding 1 to avoid divide by Zero warning\n",
    "            tf = 1 + np.log10(doc_tokens_counter[token]+1)\n",
    "            # Cosine Normalization\n",
    "            tf_idf_vector[doc_id]['tf_idf_vector'][tokens_with_index[token]] = tf / cnt\n",
    "        tf_idf_vector[doc_id]['tf_idf_vector'] = tf_idf_vector[doc_id]['tf_idf_vector'].tolist()\n",
    "    return tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function to print dictionary for initial two elements for debugging\n",
    "def print_dictionary(dictionary):\n",
    "    count = 0\n",
    "    for key, value in dictionary.items():\n",
    "        if count == 2:\n",
    "            break\n",
    "        count += 1\n",
    "        print(\"key:\", key, \"Value:\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Initialization Code\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    documents_dict = {}\n",
    "    tf_idf_vector = {}    \n",
    "    bag_of_words = {}\n",
    "    documents_title_dict = {}\n",
    "    \n",
    "    read_corpus_file(documents_dict)\n",
    "    text_preprocessing(documents_dict)\n",
    "    #print_dictionary(documents_dict)\n",
    "    make_bag_of_words(bag_of_words, documents_dict)\n",
    "    #print_dictionary(bag_of_words)\n",
    "    build_inverted_index(tf_idf_vector, documents_dict, bag_of_words)\n",
    "    #print_dictionary(tf_idf_vector)\n",
    "    with open('documents_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(documents_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('tf_idf_vector.pkl', 'wb') as handle:\n",
    "        pickle.dump(tf_idf_vector, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('bag_of_words.pkl', 'wb') as handle:\n",
    "        pickle.dump(bag_of_words, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('documents_title_dict.pkl', 'wb') as handle:\n",
    "        pickle.dump(documents_title_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
